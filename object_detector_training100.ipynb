{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mprksa/blocks/blob/main/object_detector_training100.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UnB1pBO1OTjC"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/mprksa/blocks.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GMSCcHh1LScM"
      },
      "outputs": [],
      "source": [
        "!python --version\n",
        "!pip install --upgrade pip\n",
        "!pip install mediapipe-model-maker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oazmbPzKHYFq"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "import json\n",
        "import tensorflow as tf\n",
        "assert tf.__version__.startswith('2')\n",
        "\n",
        "from mediapipe_model_maker import object_detector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zP1AkaRL72Z"
      },
      "source": [
        "# **Prepare data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mz3-eHe07FEX"
      },
      "outputs": [],
      "source": [
        "train_dataset_path = \"blocks/train\"\n",
        "validation_dataset_path = \"blocks/validation\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7TPn8Mb_aJb"
      },
      "source": [
        "# **Review dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2f_Z-TAwNK3n"
      },
      "outputs": [],
      "source": [
        "with open(os.path.join(train_dataset_path, \"labels.json\"), \"r\") as f:\n",
        "  labels_json = json.load(f)\n",
        "for category_item in labels_json[\"categories\"]:\n",
        "  print(f\"{category_item['id']}: {category_item['name']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6kTw3uodPl7-"
      },
      "outputs": [],
      "source": [
        "#@title Visualize the training dataset\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import patches, text, patheffects\n",
        "from collections import defaultdict\n",
        "import math\n",
        "\n",
        "def draw_outline(obj):\n",
        "  obj.set_path_effects([patheffects.Stroke(linewidth=4,  foreground='black'), patheffects.Normal()])\n",
        "def draw_box(ax, bb):\n",
        "  patch = ax.add_patch(patches.Rectangle((bb[0],bb[1]), bb[2], bb[3], fill=False, edgecolor='red', lw=2))\n",
        "  draw_outline(patch)\n",
        "def draw_text(ax, bb, txt, disp):\n",
        "  text = ax.text(bb[0],(bb[1]-disp),txt,verticalalignment='top'\n",
        "  ,color='white',fontsize=10,weight='bold')\n",
        "  draw_outline(text)\n",
        "def draw_bbox(ax, annotations_list, id_to_label, image_shape):\n",
        "  for annotation in annotations_list:\n",
        "    cat_id = annotation[\"category_id\"]\n",
        "    bbox = annotation[\"bbox\"]\n",
        "    draw_box(ax, bbox)\n",
        "    draw_text(ax, bbox, id_to_label[cat_id], image_shape[0] * 0.05)\n",
        "def visualize(dataset_folder, max_examples=None):\n",
        "  with open(os.path.join(dataset_folder, \"labels.json\"), \"r\") as f:\n",
        "    labels_json = json.load(f)\n",
        "  images = labels_json[\"images\"]\n",
        "  cat_id_to_label = {item[\"id\"]:item[\"name\"] for item in labels_json[\"categories\"]}\n",
        "  image_annots = defaultdict(list)\n",
        "  for annotation_obj in labels_json[\"annotations\"]:\n",
        "    image_id = annotation_obj[\"image_id\"]\n",
        "    image_annots[image_id].append(annotation_obj)\n",
        "\n",
        "  if max_examples is None:\n",
        "    max_examples = len(image_annots.items())\n",
        "  n_rows = math.ceil(max_examples / 3)\n",
        "  fig, axs = plt.subplots(n_rows, 3, figsize=(24, n_rows*8)) # 3 columns(2nd index), 8x8 for each image\n",
        "  for ind, (image_id, annotations_list) in enumerate(list(image_annots.items())[:max_examples]):\n",
        "    ax = axs[ind//3, ind%3]\n",
        "    img = plt.imread(os.path.join(dataset_folder, \"images\", images[image_id][\"file_name\"]))\n",
        "    ax.imshow(img)\n",
        "    draw_bbox(ax, annotations_list, cat_id_to_label, img.shape)\n",
        "  plt.show()\n",
        "\n",
        "visualize(train_dataset_path, 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EOdyImqyI6s-"
      },
      "outputs": [],
      "source": [
        "train_data = object_detector.Dataset.from_coco_folder(train_dataset_path, cache_dir=\"/tmp/od_data/train\")\n",
        "validation_data = object_detector.Dataset.from_coco_folder(validation_dataset_path, cache_dir=\"/tmp/od_data/validation\")\n",
        "print(\"train_data size: \", train_data.size)\n",
        "print(\"validation_data size: \", validation_data.size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODK4BN5N8Rdi"
      },
      "source": [
        "# **Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ZHjWHM1JyiN"
      },
      "outputs": [],
      "source": [
        "spec = object_detector.SupportedModels.MOBILENET_MULTI_AVG_I384\n",
        "hparams = object_detector.HParams(export_dir='exported_model')\n",
        "options = object_detector.ObjectDetectorOptions(\n",
        "    supported_model=spec,\n",
        "    hparams=hparams\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V5bIsWBZCb8d"
      },
      "outputs": [],
      "source": [
        "model = object_detector.ObjectDetector.create(\n",
        "    train_data=train_data,\n",
        "    validation_data=validation_data,\n",
        "    options=options)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tLbtkz66KQR"
      },
      "source": [
        "# **Evaluate**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T9RbNGJY6MY1"
      },
      "outputs": [],
      "source": [
        "loss, coco_metrics = model.evaluate(validation_data, batch_size=4)\n",
        "print(f\"Validation loss: {loss}\")\n",
        "print(f\"Validation coco metrics: {coco_metrics}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Grafik**"
      ],
      "metadata": {
        "id": "w5aiCn-te3-r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Data\n",
        "epochs = range(1, 31)\n",
        "train_total_loss = [7.3300, 1.0832, 0.6793, 0.4450, 0.3384, 0.2700, 0.2394, 0.2079, 0.1880, 0.1742, 0.1796, 0.1546, 0.1492, 0.1481, 0.1330, 0.1444, 0.1419, 0.1366, 0.1182, 0.1124, 0.1199, 0.1166, 0.1286, 0.1090, 0.1015, 0.0991, 0.1097, 0.0965, 0.1089, 0.0979]\n",
        "val_total_loss = [1.2431, 0.8980, 0.5816, 0.4320, 0.3684, 0.3062, 0.2955, 0.2812, 0.2674, 0.2656, 0.2597, 0.2618, 0.2617, 0.2554, 0.2498, 0.2590, 0.2549, 0.2520, 0.2564, 0.2615, 0.2498, 0.2550, 0.2665, 0.2611, 0.2585, 0.2540, 0.2546, 0.2583, 0.2738, 0.2616]\n",
        "\n",
        "train_cls_loss = [7.1613, 0.9615, 0.5766, 0.3548, 0.2554, 0.1912, 0.1625, 0.1342, 0.1156, 0.1029, 0.1078, 0.0851, 0.0803, 0.0788, 0.0653, 0.0751, 0.0721, 0.0670, 0.0515, 0.0456, 0.0521, 0.0489, 0.0591, 0.0420, 0.0354, 0.0318, 0.0412, 0.0296, 0.0405, 0.0306]\n",
        "val_cls_loss = [1.1204, 0.7906, 0.4824, 0.3346, 0.2668, 0.2165, 0.2068, 0.1901, 0.1762, 0.1767, 0.1719, 0.1733, 0.1699, 0.1677, 0.1619, 0.1695, 0.1664, 0.1626, 0.1645, 0.1646, 0.1612, 0.1648, 0.1752, 0.1698, 0.1719, 0.1679, 0.1680, 0.1700, 0.1766, 0.1733]\n",
        "\n",
        "train_box_loss = [0.0022, 0.0012, 0.0008, 0.0006, 0.0004, 0.0004, 0.0003, 0.0003, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0001, 0.0002, 0.0002, 0.0002, 0.0001, 0.0001, 0.0001, 0.0001, 0.0002, 0.0001, 0.0001, 0.0001, 0.0002, 0.0001, 0.0002, 0.0001]\n",
        "val_box_loss = [0.0012, 0.0009, 0.0008, 0.0007, 0.0008, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0005, 0.0006, 0.0006, 0.0005, 0.0005, 0.0006, 0.0006, 0.0006, 0.0006, 0.0007, 0.0006, 0.0006, 0.0006, 0.0006, 0.0005, 0.0005, 0.0005, 0.0006, 0.0007, 0.0006]\n",
        "\n",
        "train_model_loss = [7.2693, 1.0225, 0.6185, 0.3843, 0.2777, 0.2093, 0.1787, 0.1472, 0.1274, 0.1135, 0.1190, 0.0940, 0.0886, 0.0875, 0.0724, 0.0839, 0.0813, 0.0761, 0.0577, 0.0519, 0.0594, 0.0561, 0.0681, 0.0485, 0.0410, 0.0387, 0.0493, 0.0361, 0.0485, 0.0376]\n",
        "val_model_loss = [1.1824, 0.8373, 0.5208, 0.3713, 0.3077, 0.2455, 0.2348, 0.2205, 0.2067, 0.2049, 0.1991, 0.2012, 0.2011, 0.1948, 0.1893, 0.1985, 0.1943, 0.1915, 0.1959, 0.2011, 0.1894, 0.1945, 0.2060, 0.2007, 0.1981, 0.1936, 0.1942, 0.1979, 0.2135, 0.2013]\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.plot(epochs, train_total_loss, label='Training Total Loss')\n",
        "plt.plot(epochs, val_total_loss, label='Validation Total Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Total Loss')\n",
        "plt.title('Total Loss per Epoch')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.plot(epochs, train_cls_loss, label='Training Classification Loss')\n",
        "plt.plot(epochs, val_cls_loss, label='Validation Classification Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Classification Loss')\n",
        "plt.title('Classification Loss per Epoch')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(2, 2, 3)\n",
        "plt.plot(epochs, train_box_loss, label='Training Box Loss')\n",
        "plt.plot(epochs, val_box_loss, label='Validation Box Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Box Loss')\n",
        "plt.title('Box Loss per Epoch')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(2, 2, 4)\n",
        "plt.plot(epochs, train_model_loss, label='Training Model Loss')\n",
        "plt.plot(epochs, val_model_loss, label='Validation Model Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Model Loss')\n",
        "plt.title('Model Loss per Epoch')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "4FBcwv9se9yE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Data training dan validasi\n",
        "epochs = range(1, 31)\n",
        "train_total_loss = [7.3300, 1.0832, 0.6793, 0.4450, 0.3384, 0.2700, 0.2394, 0.2079, 0.1880, 0.1742, 0.1796, 0.1546, 0.1492, 0.1481, 0.1330, 0.1444, 0.1419, 0.1366, 0.1182, 0.1124, 0.1199, 0.1166, 0.1286, 0.1090, 0.1015, 0.0991, 0.1097, 0.0965, 0.1089, 0.0979]\n",
        "val_total_loss = [1.2431, 0.8980, 0.5816, 0.4320, 0.3684, 0.3062, 0.2955, 0.2812, 0.2674, 0.2656, 0.2597, 0.2618, 0.2617, 0.2554, 0.2498, 0.2590, 0.2549, 0.2520, 0.2564, 0.2615, 0.2498, 0.2550, 0.2665, 0.2611, 0.2585, 0.2540, 0.2546, 0.2583, 0.2738, 0.2616]\n",
        "train_cls_loss = [7.1613, 0.9615, 0.5766, 0.3548, 0.2554, 0.1912, 0.1625, 0.1342, 0.1156, 0.1029, 0.1078, 0.0851, 0.0803, 0.0788, 0.0653, 0.0751, 0.0721, 0.0670, 0.0515, 0.0456, 0.0521, 0.0489, 0.0591, 0.0420, 0.0354, 0.0318, 0.0412, 0.0296, 0.0405, 0.0306]\n",
        "val_cls_loss = [1.1204, 0.7906, 0.4824, 0.3346, 0.2668, 0.2165, 0.2068, 0.1901, 0.1762, 0.1767, 0.1719, 0.1733, 0.1699, 0.1677, 0.1619, 0.1695, 0.1664, 0.1626, 0.1645, 0.1646, 0.1612, 0.1648, 0.1752, 0.1698, 0.1719, 0.1679, 0.1680, 0.1700, 0.1766, 0.1733]\n",
        "train_box_loss = [0.0022, 0.0012, 0.0008, 0.0006, 0.0004, 0.0004, 0.0003, 0.0003, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0001, 0.0002, 0.0002, 0.0002, 0.0001, 0.0001, 0.0001, 0.0001, 0.0002, 0.0001, 0.0001, 0.0001, 0.0002, 0.0001, 0.0002, 0.0001]\n",
        "val_box_loss = [0.0012, 0.0009, 0.0008, 0.0007, 0.0008, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0005, 0.0006, 0.0006, 0.0005, 0.0005, 0.0006, 0.0006, 0.0006, 0.0006, 0.0007, 0.0006, 0.0006, 0.0006, 0.0006, 0.0005, 0.0005, 0.0005, 0.0006, 0.0007, 0.0006]\n",
        "\n",
        "# Plot total loss\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(epochs, train_total_loss, label='Train Total Loss')\n",
        "plt.plot(epochs, val_total_loss, label='Val Total Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Total Loss per Epoch')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Plot cls loss\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(epochs, train_cls_loss, label='Train Classification Loss')\n",
        "plt.plot(epochs, val_cls_loss, label='Val Classification Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Classification Loss per Epoch')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Plot box loss\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(epochs, train_box_loss, label='Train Box Loss')\n",
        "plt.plot(epochs, val_box_loss, label='Val Box Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Box Loss per Epoch')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "3B35W1zKfBPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LAOdfyxmmzS"
      },
      "source": [
        "# **Export Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PWZqnGEKVP13"
      },
      "outputs": [],
      "source": [
        "model.export_model()\n",
        "!ls exported_model\n",
        "files.download('exported_model/model.tflite')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcYu5ENbT4T6"
      },
      "source": [
        "## **Model quantization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_7nRSQT9WCS-"
      },
      "outputs": [],
      "source": [
        "qat_hparams = object_detector.QATHParams(learning_rate=0.1, batch_size=4, epochs=30, decay_steps=12, decay_rate=0.99)\n",
        "model.quantization_aware_training(train_data, validation_data, qat_hparams=qat_hparams)\n",
        "qat_loss, qat_coco_metrics = model.evaluate(validation_data)\n",
        "print(f\"QAT validation loss: {qat_loss}\")\n",
        "print(f\"QAT validation coco metrics: {qat_coco_metrics}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rsixePxCWJDp"
      },
      "outputs": [],
      "source": [
        "model.export_model('model_int8_qat.tflite')\n",
        "!ls -lh exported_model\n",
        "files.download('exported_model/model_int8_qat.tflite')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eO8nZR3Cgx8_"
      },
      "source": [
        "#**Post-training quantization (fp32 quantization)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yo7cQ_N-ZE8A"
      },
      "outputs": [],
      "source": [
        "from mediapipe_model_maker import model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kbwc3g_Fa3dv"
      },
      "outputs": [],
      "source": [
        "quantization_config = quantization.QuantizationConfig.for_float32()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mmzEu_AjbMPI"
      },
      "outputs": [],
      "source": [
        "model.restore_float_ckpt()\n",
        "model.export_model(model_name=\"model_fp32.tflite\", quantization_config=quantization_config)\n",
        "!ls -lh exported_model\n",
        "files.download('exported_model/model_fp32.tflite')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HCrUl8z6liX"
      },
      "source": [
        "## Benchmarking\n",
        "Below is a summary of our benchmarking results for the supported model architectures. These models were trained and evaluated on the same android figurines dataset as this notebook. When considering the model benchmarking results, there are a few important caveats to keep in mind:\n",
        "* The android figurines dataset is a small and simple dataset with 62 training examples and 10 validation examples. Since the dataset is quite small, metrics may vary drastically due to variances in the training process. This dataset was provided for demo purposes and it is recommended to collect more data samples for better performing models.\n",
        "* The float32 models were trained with the default HParams, and the QAT step for the int8 models was run with `QATHParams(learning_rate=0.1, batch_size=4, epochs=30, decay_rate=1)`.\n",
        "* For your own dataset, you will likely need to tune values for both HParams and QATHParams in order to achieve the best results. See the [Hyperparameters](#hyperparameters) section above for more information on configuring training parameters.\n",
        "* All latency numbers are benchmarked on the Pixel 6.\n",
        "\n",
        "\n",
        "<table>\n",
        "<thead>\n",
        "<col>\n",
        "<col>\n",
        "<colgroup span=\"2\"></colgroup>\n",
        "<colgroup span=\"2\"></colgroup>\n",
        "<colgroup span=\"2\"></colgroup>\n",
        "<tr>\n",
        "<th rowspan=\"2\">Model architecture</th>\n",
        "<th rowspan=\"2\">Input Image Size</th>\n",
        "<th colspan=\"2\" scope=\"colgroup\">Test AP</th>\n",
        "<th colspan=\"2\" scope=\"colgroup\">CPU Latency</th>\n",
        "<th colspan=\"2\" scope=\"colgroup\">Model Size</th>\n",
        "</tr>\n",
        "<tr>\n",
        "<th>float32</th>\n",
        "<th>QAT int8</th>\n",
        "<th>float32</th>\n",
        "<th>QAT int8</th>\n",
        "<th>float32</th>\n",
        "<th>QAT int8</th>\n",
        "</tr>\n",
        "</thead>\n",
        "<tbody>\n",
        "<tr>\n",
        "<td>MobileNetV2</td>\n",
        "<td>256x256</td>\n",
        "<td>88.4%</td>\n",
        "<td>73.5%</td>\n",
        "<td>48ms</td>\n",
        "<td>16ms</td>\n",
        "<td>11MB</td>\n",
        "<td>3.2MB</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>MobileNetV2 I320</td>\n",
        "<td>320x320</td>\n",
        "<td>89.1%</td>\n",
        "<td>75.5%</td>\n",
        "<td>75ms</td>\n",
        "<td>33.38ms</td>\n",
        "<td>10MB</td>\n",
        "<td>3.3MB</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>MobileNet MultiHW AVG</td>\n",
        "<td>256x256</td>\n",
        "<td>88.5%</td>\n",
        "<td>70.0%</td>\n",
        "<td>56ms</td>\n",
        "<td>19ms</td>\n",
        "<td>13MB</td>\n",
        "<td>3.6MB</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>MobileNet MultiHW AVG I384</td>\n",
        "<td>384x384</td>\n",
        "<td>92.7%</td>\n",
        "<td>73.4%</td>\n",
        "<td>238ms</td>\n",
        "<td>41ms</td>\n",
        "<td>13MB</td>\n",
        "<td>3.6MB</td>\n",
        "</tr>\n",
        "\n",
        "</tbody>\n",
        "</table>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TOCPzKohXxy6"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}