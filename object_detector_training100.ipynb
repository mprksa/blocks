{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mprksa/blocks/blob/main/object_detector_training100.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UnB1pBO1OTjC"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/mprksa/blocks.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GMSCcHh1LScM"
      },
      "outputs": [],
      "source": [
        "!python --version\n",
        "!pip install --upgrade pip\n",
        "!pip install mediapipe-model-maker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oazmbPzKHYFq"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "import json\n",
        "import tensorflow as tf\n",
        "assert tf.__version__.startswith('2')\n",
        "\n",
        "from mediapipe_model_maker import object_detector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zP1AkaRL72Z"
      },
      "source": [
        "# **Prepare data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mz3-eHe07FEX"
      },
      "outputs": [],
      "source": [
        "train_dataset_path = \"blocks/train\"\n",
        "validation_dataset_path = \"blocks/validation\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7TPn8Mb_aJb"
      },
      "source": [
        "# **Review dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2f_Z-TAwNK3n"
      },
      "outputs": [],
      "source": [
        "with open(os.path.join(train_dataset_path, \"labels.json\"), \"r\") as f:\n",
        "  labels_json = json.load(f)\n",
        "for category_item in labels_json[\"categories\"]:\n",
        "  print(f\"{category_item['id']}: {category_item['name']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6kTw3uodPl7-"
      },
      "outputs": [],
      "source": [
        "#@title Visualize the training dataset\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import patches, text, patheffects\n",
        "from collections import defaultdict\n",
        "import math\n",
        "\n",
        "def draw_outline(obj):\n",
        "  obj.set_path_effects([patheffects.Stroke(linewidth=4,  foreground='black'), patheffects.Normal()])\n",
        "def draw_box(ax, bb):\n",
        "  patch = ax.add_patch(patches.Rectangle((bb[0],bb[1]), bb[2], bb[3], fill=False, edgecolor='red', lw=2))\n",
        "  draw_outline(patch)\n",
        "def draw_text(ax, bb, txt, disp):\n",
        "  text = ax.text(bb[0],(bb[1]-disp),txt,verticalalignment='top'\n",
        "  ,color='white',fontsize=10,weight='bold')\n",
        "  draw_outline(text)\n",
        "def draw_bbox(ax, annotations_list, id_to_label, image_shape):\n",
        "  for annotation in annotations_list:\n",
        "    cat_id = annotation[\"category_id\"]\n",
        "    bbox = annotation[\"bbox\"]\n",
        "    draw_box(ax, bbox)\n",
        "    draw_text(ax, bbox, id_to_label[cat_id], image_shape[0] * 0.05)\n",
        "def visualize(dataset_folder, max_examples=None):\n",
        "  with open(os.path.join(dataset_folder, \"labels.json\"), \"r\") as f:\n",
        "    labels_json = json.load(f)\n",
        "  images = labels_json[\"images\"]\n",
        "  cat_id_to_label = {item[\"id\"]:item[\"name\"] for item in labels_json[\"categories\"]}\n",
        "  image_annots = defaultdict(list)\n",
        "  for annotation_obj in labels_json[\"annotations\"]:\n",
        "    image_id = annotation_obj[\"image_id\"]\n",
        "    image_annots[image_id].append(annotation_obj)\n",
        "\n",
        "  if max_examples is None:\n",
        "    max_examples = len(image_annots.items())\n",
        "  n_rows = math.ceil(max_examples / 3)\n",
        "  fig, axs = plt.subplots(n_rows, 3, figsize=(24, n_rows*8)) # 3 columns(2nd index), 8x8 for each image\n",
        "  for ind, (image_id, annotations_list) in enumerate(list(image_annots.items())[:max_examples]):\n",
        "    ax = axs[ind//3, ind%3]\n",
        "    img = plt.imread(os.path.join(dataset_folder, \"images\", images[image_id][\"file_name\"]))\n",
        "    ax.imshow(img)\n",
        "    draw_bbox(ax, annotations_list, cat_id_to_label, img.shape)\n",
        "  plt.show()\n",
        "\n",
        "visualize(train_dataset_path, 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EOdyImqyI6s-"
      },
      "outputs": [],
      "source": [
        "train_data = object_detector.Dataset.from_coco_folder(train_dataset_path, cache_dir=\"/tmp/od_data/train\")\n",
        "validation_data = object_detector.Dataset.from_coco_folder(validation_dataset_path, cache_dir=\"/tmp/od_data/validation\")\n",
        "print(\"train_data size: \", train_data.size)\n",
        "print(\"validation_data size: \", validation_data.size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODK4BN5N8Rdi"
      },
      "source": [
        "# **Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ZHjWHM1JyiN"
      },
      "outputs": [],
      "source": [
        "spec = object_detector.SupportedModels.MOBILENET_MULTI_AVG_I384\n",
        "hparams = object_detector.HParams(export_dir='exported_model')\n",
        "options = object_detector.ObjectDetectorOptions(\n",
        "    supported_model=spec,\n",
        "    hparams=hparams\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V5bIsWBZCb8d"
      },
      "outputs": [],
      "source": [
        "model = object_detector.ObjectDetector.create(\n",
        "    train_data=train_data,\n",
        "    validation_data=validation_data,\n",
        "    options=options)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tLbtkz66KQR"
      },
      "source": [
        "# **Evaluate**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T9RbNGJY6MY1"
      },
      "outputs": [],
      "source": [
        "loss, coco_metrics = model.evaluate(validation_data, batch_size=4)\n",
        "print(f\"Validation loss: {loss}\")\n",
        "print(f\"Validation coco metrics: {coco_metrics}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LAOdfyxmmzS"
      },
      "source": [
        "# **Export Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PWZqnGEKVP13"
      },
      "outputs": [],
      "source": [
        "model.export_model()\n",
        "!ls exported_model\n",
        "files.download('exported_model/model.tflite')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HCrUl8z6liX"
      },
      "source": [
        "## Benchmarking\n",
        "Below is a summary of our benchmarking results for the supported model architectures. These models were trained and evaluated on the same android figurines dataset as this notebook. When considering the model benchmarking results, there are a few important caveats to keep in mind:\n",
        "* The android figurines dataset is a small and simple dataset with 62 training examples and 10 validation examples. Since the dataset is quite small, metrics may vary drastically due to variances in the training process. This dataset was provided for demo purposes and it is recommended to collect more data samples for better performing models.\n",
        "* The float32 models were trained with the default HParams, and the QAT step for the int8 models was run with `QATHParams(learning_rate=0.1, batch_size=4, epochs=30, decay_rate=1)`.\n",
        "* For your own dataset, you will likely need to tune values for both HParams and QATHParams in order to achieve the best results. See the [Hyperparameters](#hyperparameters) section above for more information on configuring training parameters.\n",
        "* All latency numbers are benchmarked on the Pixel 6.\n",
        "\n",
        "\n",
        "<table>\n",
        "<thead>\n",
        "<col>\n",
        "<col>\n",
        "<colgroup span=\"2\"></colgroup>\n",
        "<colgroup span=\"2\"></colgroup>\n",
        "<colgroup span=\"2\"></colgroup>\n",
        "<tr>\n",
        "<th rowspan=\"2\">Model architecture</th>\n",
        "<th rowspan=\"2\">Input Image Size</th>\n",
        "<th colspan=\"2\" scope=\"colgroup\">Test AP</th>\n",
        "<th colspan=\"2\" scope=\"colgroup\">CPU Latency</th>\n",
        "<th colspan=\"2\" scope=\"colgroup\">Model Size</th>\n",
        "</tr>\n",
        "<tr>\n",
        "<th>float32</th>\n",
        "<th>QAT int8</th>\n",
        "<th>float32</th>\n",
        "<th>QAT int8</th>\n",
        "<th>float32</th>\n",
        "<th>QAT int8</th>\n",
        "</tr>\n",
        "</thead>\n",
        "<tbody>\n",
        "<tr>\n",
        "<td>MobileNetV2</td>\n",
        "<td>256x256</td>\n",
        "<td>88.4%</td>\n",
        "<td>73.5%</td>\n",
        "<td>48ms</td>\n",
        "<td>16ms</td>\n",
        "<td>11MB</td>\n",
        "<td>3.2MB</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>MobileNetV2 I320</td>\n",
        "<td>320x320</td>\n",
        "<td>89.1%</td>\n",
        "<td>75.5%</td>\n",
        "<td>75ms</td>\n",
        "<td>33.38ms</td>\n",
        "<td>10MB</td>\n",
        "<td>3.3MB</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>MobileNet MultiHW AVG</td>\n",
        "<td>256x256</td>\n",
        "<td>88.5%</td>\n",
        "<td>70.0%</td>\n",
        "<td>56ms</td>\n",
        "<td>19ms</td>\n",
        "<td>13MB</td>\n",
        "<td>3.6MB</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>MobileNet MultiHW AVG I384</td>\n",
        "<td>384x384</td>\n",
        "<td>92.7%</td>\n",
        "<td>73.4%</td>\n",
        "<td>238ms</td>\n",
        "<td>41ms</td>\n",
        "<td>13MB</td>\n",
        "<td>3.6MB</td>\n",
        "</tr>\n",
        "\n",
        "</tbody>\n",
        "</table>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TOCPzKohXxy6"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}